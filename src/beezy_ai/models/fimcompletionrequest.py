"""Code generated by Speakeasy (https://speakeasyapi.dev). DO NOT EDIT."""

from __future__ import annotations
from beezy_ai.types import BaseModel, Nullable
from pydantic import model_serializer
from typing import List, Optional, TypedDict, Union
from typing_extensions import NotRequired


class FIMCompletionRequestTypedDict(TypedDict):
    prompt: str
    r"""The text/code to complete."""
    model: Nullable[str]
    r"""ID of the model to use. Only compatible for now with:
    - `codestral-2405`
    - `codestral-latest`

    """
    suffix: NotRequired[Nullable[str]]
    r"""Optional text/code that adds more context for the model.
    When given a `prompt` and a `suffix` the model will fill
    what is between them. When `suffix` is not provided, the
    model will simply execute completion starting with
    `prompt`.

    """
    temperature: NotRequired[Nullable[float]]
    r"""What sampling temperature to use, between 0.0 and 1.0.
    Higher values like 0.8 will make the outptu more random,
    while lower values like 0.2 will make it more focused and
    deterministic.

    We generally recommend altering this or `top_p` but not both.

    """
    top_p: NotRequired[Nullable[float]]
    r"""Nucleus sampling, where the model considers the results of the
    tokens with with `top_p` probability mass. So 0.1 means only
    the tokens comprising the top 10% probability mass are considered.

    We generally recommend altering this or `temperature` but not both.

    """
    max_tokens: NotRequired[Nullable[int]]
    r"""The maximum number of tokens to generate in the completion.

    The token count of your prompt plus `max_tokens` cannot
    exceed the model's context length.

    """
    min_tokens: NotRequired[Nullable[int]]
    r"""The minimum number of tokens to generate in the completion.

    """
    stream: NotRequired[bool]
    r"""Whether to stream back partial progress. If set, tokens will be
    sent as data-only server-side events as they become available,
    with the stream terminated by a data: [DONE] message.\" 
    Otherwise, the server will hold the request open until the timeout
    or until completion, with the response containing the full result
    as JSON.

    """
    random_seed: NotRequired[Nullable[int]]
    r"""The seed to use for random sampling. If set, different calls will
    generate deterministic results.

    """
    stop: NotRequired[StopTypedDict]
    

class FIMCompletionRequest(BaseModel):
    prompt: str
    r"""The text/code to complete."""
    model: Nullable[str]
    r"""ID of the model to use. Only compatible for now with:
    - `codestral-2405`
    - `codestral-latest`

    """
    suffix: Optional[Nullable[str]] = None
    r"""Optional text/code that adds more context for the model.
    When given a `prompt` and a `suffix` the model will fill
    what is between them. When `suffix` is not provided, the
    model will simply execute completion starting with
    `prompt`.

    """
    temperature: Optional[Nullable[float]] = 0.7
    r"""What sampling temperature to use, between 0.0 and 1.0.
    Higher values like 0.8 will make the outptu more random,
    while lower values like 0.2 will make it more focused and
    deterministic.

    We generally recommend altering this or `top_p` but not both.

    """
    top_p: Optional[Nullable[float]] = 1
    r"""Nucleus sampling, where the model considers the results of the
    tokens with with `top_p` probability mass. So 0.1 means only
    the tokens comprising the top 10% probability mass are considered.

    We generally recommend altering this or `temperature` but not both.

    """
    max_tokens: Optional[Nullable[int]] = None
    r"""The maximum number of tokens to generate in the completion.

    The token count of your prompt plus `max_tokens` cannot
    exceed the model's context length.

    """
    min_tokens: Optional[Nullable[int]] = None
    r"""The minimum number of tokens to generate in the completion.

    """
    stream: Optional[bool] = False
    r"""Whether to stream back partial progress. If set, tokens will be
    sent as data-only server-side events as they become available,
    with the stream terminated by a data: [DONE] message.\" 
    Otherwise, the server will hold the request open until the timeout
    or until completion, with the response containing the full result
    as JSON.

    """
    random_seed: Optional[Nullable[int]] = None
    r"""The seed to use for random sampling. If set, different calls will
    generate deterministic results.

    """
    stop: Optional[Stop] = None
    
    @model_serializer(mode="wrap")
    def serialize_model(self, handler):
        optional_fields = ["suffix", "temperature", "top_p", "max_tokens", "min_tokens", "stream", "random_seed", "stop"]
        nullable_fields = ["model", "suffix", "temperature", "top_p", "max_tokens", "min_tokens", "random_seed"]
        null_default_fields = []

        serialized = handler(self)

        m = {}

        for n, f in self.model_fields.items():
            k = f.alias or n
            val = serialized.get(k)

            if val is not None:
                m[k] = val
            elif not k in optional_fields or (
                    k in optional_fields
                    and k in nullable_fields
                    and (self.__pydantic_fields_set__.intersection({n}) or k in null_default_fields) # pylint: disable=no-member
                ):
                m[k] = val

        return m
        

StopTypedDict = Union[str, List[str]]


Stop = Union[str, List[str]]

